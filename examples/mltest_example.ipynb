{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fae219c",
   "metadata": {},
   "source": [
    "# Example usage of MLTest\n",
    "\n",
    "This notebook shows you how to get started with MLTest. \n",
    "\n",
    "You'll see how to evaluate a state-of-the-art YOLO model on a COCO dataset using MLTest. \n",
    "\n",
    "We will also launch the MLTest dashboard and introduce you to MLTest's programmatic API which allows easy customization for specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from rich.console import Console\n",
    "import warnings\n",
    "\n",
    "from evaluator import Evaluator, start_dashboard, stop_dashboard\n",
    "from lakera.config import Config\n",
    "from lakera.test_pipeline import TestPipeline\n",
    "import lakera.util as util\n",
    "\n",
    "console = Console()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd48b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_OUTPUT = \"./mltest_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc8ee5",
   "metadata": {},
   "source": [
    "## Run MLTest\n",
    "\n",
    "In this section you get to evaluate a YOLO model on a COCO dataset using MLTest.\n",
    "\n",
    "Let's start by setting up the MLTest `Evaluator` object with a default configuration. The `Evaluator` and the MLTest Dashboard communicate through a shared folder which is specified with `PATH_TO_OUTPUT`. Executing the next couple of lines will execute MLTest's default test suite (performance, robustness, and data tests). It may take a few seconds to complete, please be patient üôè."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba5e70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(PATH_TO_OUTPUT)\n",
    "\n",
    "evaluator.load_dataset(\"coco\")\n",
    "evaluator.load_model(\"yolo\")\n",
    "evaluator.load_default_config()\n",
    "\n",
    "# You're all setup, simply execute the evaluation. \n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af628536",
   "metadata": {},
   "source": [
    "## Start the Lakera dashboard\n",
    "\n",
    "Once MLTest has completed its evaluation, you can see the results quickly using the MLTest Dashboard. \n",
    "\n",
    "Execute the following cell to initialize the dashboard with the results from the previous step.\n",
    "\n",
    "Please make sure that Docker is installed and running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47932e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_id = start_dashboard(PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6955ae",
   "metadata": {},
   "source": [
    "## Add additional tests\n",
    "\n",
    "Using MLTest's programmatic Python API, it is very simple to add additional tests to your evaluator and re-run it. \n",
    "\n",
    "In this example, we add robustness tests that test the model against realistic environmental changes that can severely impact performance in production. \n",
    "\n",
    "Once the run has finished, you should see the run in the Dashboard after a minute. Be sure to use the comparison feature to compare with the previous run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakera.transforms import Brightness, PackageLoss, LocalContrastEnhancement\n",
    "\n",
    "evaluator.add_robustness_test(Brightness())\n",
    "evaluator.add_robustness_test(PackageLoss())\n",
    "evaluator.add_robustness_test(LocalContrastEnhancement())\n",
    "\n",
    "# Run MLTest\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_dashboard(d_id)\n",
    "d_id = start_dashboard(PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae3915",
   "metadata": {},
   "source": [
    "## Add regression sets on the fly\n",
    "\n",
    "Many times, we want to keep track of regression datasets to make sure that our models don't fail where it matters the most. \n",
    "\n",
    "With MLTest, it's very easy to add regression sets on the fly and automate testing on these as well. The following step adds a regression dataset inline and re-runs MLTest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ba33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.add_dataset(\n",
    "    name_tag=\"first_regression_set\",\n",
    "    compute_invariance=True,\n",
    "    batch_size=5,\n",
    "    path_to_dataset=\"data/coco/coco_B\"\n",
    ")\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2093b",
   "metadata": {},
   "source": [
    "Don't forget to check out the results on the dashboard.\n",
    "\n",
    "This is the end of this quick tutorial. But stay tuned ‚Äì we are already working on extending it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_dashboard(d_id)\n",
    "d_id = start_dashboard(PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185ec61",
   "metadata": {},
   "source": [
    "## Stop the Dashboard\n",
    "\n",
    "Once you are done exploring the dashboard, make sure to stop it from running in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_dashboard(d_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
